# -*- coding: utf-8 -*-
"""langdeck_sync_stable.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fnNcy5VyYoexMnV9o6-kRgrFzcy5dWdc

# Langdeck Synchro de bout en bout
"""

# Commented out IPython magic to ensure Python compatibility.
#!/usr/bin/env python
# -*- coding: utf8 -*-
import pandas as pd
pd.set_option("display.width",1000)
pd.options.mode.chained_assignment = None  # default='warn'

import numpy as np
import requests
import json
import uuid
from datetime import datetime
import datetime
import re
import string
import unicodedata

# Gspread  API for Google sheets
!pip install gspread --upgrade
# fix here => https://stackoverflow.com/questions/71347973/modulenotfounderror-no-module-named-gspread-models
!pip uninstall -y gspread-dataframe
!pip install gspread-dataframe
!pip install gspread-formatting

#==== Google Drive 
from google.colab import drive
drive.mount('/content/drive')
# %cd /content/drive/MyDrive/Trad-Union/Corpus/ASAMLA
from google.colab import files
from google.colab import auth
auth.authenticate_user()

#====== Google Credentials ================
from oauth2client.client import GoogleCredentials
# fix here => https://github.com/burnash/gspread/issues/1014
from google.auth import default
creds, _ = default()

#====== GSpread is a Python API for Google Sheets.================
import gspread
from gspread_dataframe import get_as_dataframe, set_with_dataframe
from gspread_formatting import *
gc = gspread.authorize(creds)

#====== Airtable ================
!pip install airtable-python-wrapper
from airtable import Airtable

#====== Deep Translate ================
!pip install -U deep-translator
!pip install --upgrade deepl

#====== Dictionaries ================
# larousse API
!pip install larousse-api-sunbro

#====== Lemmatizers ================
# https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer
!pip install git+https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer.git
from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer
lemmatizer = FrenchLefffLemmatizer()

!export PYTHONIOENCODING=utf8

"""# Configuration

# Functions
"""

def save_df_to_gsheet (table_name, df):
  """ 
  ------------------------------------------
  Saves a dataframe into a spreadsheet
  Parameters :
  - table_name : str
      The name of the spredsheet's tab
  - df : str
      The name of the dataframe

  Returns :
  - Result : boolean
      True if okay
      Else False    

  usage : save_df_to_gsheet (table_name, df):
  ------------------------------------------
  """  
  try:
    t_ = wb.worksheet(table_name)
    wb.del_worksheet(t_)
  except:
      print ("Onglet inexistant !")
      return False

  wb.add_worksheet(table_name, 1, 1)
  export_sheet = wb.worksheet(table_name)
  set_with_dataframe(export_sheet, df)
  return True

def load_df_from_gsheet (table_name):
  try:
    tbl = wb.worksheet(table_name)
    data = tbl.get_all_values()
    df = pd.DataFrame(data[1:], columns=data[0])
  except:
      print ("Table inexistante !")
  return df
# usage : df = load_df_from_gsheet (table_name):

def get_airt_corpus():
  """ 
  ------------------------------------------
  Récupère un Dataframe du Corpus (référentiel) Airtable
  Parameters : aucun

  Returns : aucun
 
  usage : get_airt_corpus():
  ------------------------------------------
  """ 
  vk_at = airtable.get_all(view='Admin',sort='vocabulary_unit')
  df_vkat = pd.DataFrame.from_records((r['fields'] for r in vk_at))
  # on ne garde que les colonnes utiles
  df_vkat = df_vkat[["uid","lemma","rank","vocabulary_unit","headword","pivot","pivot_en","alphabet","theme","timestamp","pending"]]
  # on ajoute les colonnes de langues (absentes dans Airtable, gérées dans GSheet)
  for language in vk_languages:
    df_vkat[language["trigramme"]] = np.nan
  return df_vkat

def get_lgdk_corpus():
  df = load_df_from_gsheet ("t_corpus_all_entries")
  df['uid'] = df['hash'].str[:8]
  df['ordre'] = pd.to_numeric(df['ordre'])
  df['timestamp'] = df['timestamp'].astype('datetime64[s]')
  df['unixtime'] = df['timestamp'].astype('datetime64[s]').astype('int')
  return df


def get_list_languages():
  df_languages = load_df_from_gsheet ("t_on_languages")
  # on convertit en liste de dict
  vk_languages = json.loads(df_languages.to_json(orient="records"))
  for dic in vk_languages:
    # cast de chaine de liste en liste
    dic['vendor-code']=list(eval((dic['vendor-code'])))
  return vk_languages

def get_translation_fr_en (input):
  from deep_translator import GoogleTranslator
  return GoogleTranslator(source="fr", target="en").translate(input)

def set_airt_pivot_fr(input):
  record = airtable.search('vocabulary_unit', input)
  aDict = {}
  # on met à jour la table dans Airtable
  for i in record:
    aDict["pivot"] = i["fields"]["vocabulary_unit"]
    print (aDict["pivot"])
    airtable.update(i["id"], aDict)   
  return aDict["pivot"]

def set_airt_pivot_en(input):
  record = airtable.search('vocabulary_unit', input)
  aDict = {}
  aDict["pivot_en"] = get_translation_fr_en(input)
  print (aDict["pivot_en"])
  # on met à jour la table dans Airtable
  for i in record:
    airtable.update(i["id"], aDict)   
  return aDict["pivot_en"]
  
def create_uid (vk_uid, record):
  """ 
  ------------------------------------------
  Vérifie si le code uid est renseigné ; si non : on le crée
  Parameters : 
  - vk_uid : liste des uids
  - record : enregistrement en cours (curseur)

  Returns : aucun
 
  usage : create_uid(vk_uid, record)
  ------------------------------------------
  """   
  try:
    value = record['fields']['uid']
  except KeyError:
    # si uid manquant
    print ("mssing uid")
    # tant qu'on n'a pas un id unique (loterie)
    while True:
      auid = str(uuid.uuid4())[:8]
      # on crée l'uid
      if auid not in vk_uid:
        aDict = {}
        aDict["uid"] = auid
        print (record['id'] + " " + aDict["uid"])
        airtable.update(record['id'], aDict)
        # on met à jour la liste des uids
        vk_uid.append(auid) 
        break 
  finally:
    return

def check_missing_uids():
  """ 
  ------------------------------------------
  Vérifie si le code uid est renseigné ; si non : on le crée
  Parameters : aucun

  Returns : aucun
 
  usage : check_missing_uids():
  ------------------------------------------
  """ 
  # liste des uids en mémoire (chargé dans le DF)
  vk_uid = df_vkat['uid'].tolist()
  # on parcourt la table Airtable et on vérifie enregistrement par enregistrement
  for page in airtable.get_iter(view='Admin',sort='vocabulary_unit'):
    for record in page:
      # appel fonction
      create_uid (vk_uid, record)

def update_lgdk_corpus(hash):
  """ 
  ------------------------------------------
  Mise à jour des données GSheet qui ont pu être modifiées dans le référentiel Airtable
  Parameters : hash

  Returns : aucun
 
  usage : update_lgdk_corpus(hash):
  ------------------------------------------
  """ 
  try:
    idx = df_airt_corpus.loc[df_airt_corpus["hash"]==hash].index
    r1 = df_airt_corpus.iloc[idx]["theme_1"].values.item()
    r2 = df_airt_corpus.iloc[idx]["theme_2"].values.item()
    r3 = df_airt_corpus.iloc[idx]["theme_3"].values.item()    
    r4 = df_airt_corpus.iloc[idx]["maitre"].values.item()    
    r5 = df_airt_corpus.iloc[idx]["ordre"].values.item()    
    r6 = df_airt_corpus.iloc[idx]["expression"].values.item()    
    r7 = df_airt_corpus.iloc[idx]["idx"].values.item()    
    r8 = df_airt_corpus.iloc[idx]["translation_pivot_fr"].values.item()    
    r9 = df_airt_corpus.iloc[idx]["translation_pivot_en"].values.item()    
  except Exception as e:        
    pass  
  return pd.Series([r1, r2, r3, r4, r5, r6, r7, r8, r9])


def check_stats():
  ct_corpus = len(df_lgdk_corpus)
  # df_lgdk_corpus.loc[pd.isna(df_lgdk_corpus["translation_pivot"])]
  ct_pivot_missing = len(df_lgdk_corpus.loc[df_lgdk_corpus["translation_pivot"]==""])
  ct_pivot_fr_missing = len(df_lgdk_corpus.loc[df_lgdk_corpus["translation_pivot_fr"]==""])
  ct_pivot_en_missing = len(df_lgdk_corpus.loc[df_lgdk_corpus["translation_pivot_en"]==""])
  ct_pivot_uid_missing = len(df_lgdk_corpus.loc[df_lgdk_corpus["hash"]==""])
  s = f"Pivot(s) manquant(s) : {ct_pivot_missing}\nPivot(s) manquant(s) (FR) : {ct_pivot_fr_missing}\n"
  s += f"Pivot(s) manquant(s) (EN) : {ct_pivot_en_missing}\nUIds(s) manquant(s) : {ct_pivot_uid_missing}\n"
  print (s)
  return

def get_google_translation (expression_pivot, lang_trigramme, lang_filter, iso_code, ai_code, langue_pivot):
  from deep_translator import GoogleTranslator
  translation = expression_pivot
  state = 0 #par défaut
  if lang_trigramme == lang_filter:
    try:
      translation = GoogleTranslator(source=langue_pivot, target=iso_code).translate(expression_pivot)
      if translation != "":
        print (expression_pivot + "("+ langue_pivot +") "+ " : " + translation)
        state = 0 #found
    except Exception as e:
      print (e)
      translation = ""
      pass
  else:
    ai_code = 0
  return translation, state, ai_code

def get_deepl_translation (expression_pivot, lang_trigramme, lang_filter, iso_code, ai_code, langue_pivot):
  from deep_translator import DeeplTranslator
  translation = expression_pivot
  state = 0 #par défaut
  if lang_trigramme == lang_filter:
    try:
      translation = DeeplTranslator(api_key="ea9751e0-a036-d65e-14af-3169e96b11e4:fx", source=langue_pivot, target=iso_code, use_free_api=True).translate(expression_pivot)
      if translation != "":
        print (expression_pivot + "("+ langue_pivot +") "+ " : " + translation)
        state = 0 #found
    except Exception as e:
      print (e)
      translation = ""
      pass
  else:
    ai_code = 0
  return translation, state, ai_code

# Comparaison des versions de référentiels
def getLastModifiedTime (wb):
  # https://stackoverflow.com/questions/64254039/python-how-to-check-if-a-google-spreadsheets-was-updated
  # https://stackoverflow.com/questions/52260789/update-googlesheet-cell-with-timestamp-from-python
  revisions_uri = f'https://www.googleapis.com/drive/v3/files/{wb.id}/revisions'
  headers = {'Authorization': f'Bearer {GoogleCredentials.get_application_default().get_access_token().access_token}'}
  response = requests.get(revisions_uri, headers=headers).json()
  return response['revisions'][-1]['modifiedTime']    

def txtai_alpha (input, iso_code):
  try:
    translation = translate(input, iso_code)
  except Exception as e:
    translation = ""
    pass
  return translation, 2



def fetch_pivot_en (txt_fr):
  try:
    translation = translate(txt_fr, "en")
    print (translation)
  except Exception as e:
    translation = ""
    pass  
  return translation


def compare_langdeck_time(hash, time):
  ts1 = time
  ts2 = df_langdeck.loc[df_langdeck["hash"]==hash]["unixtime"].values[0]
  return ts1 > ts2

def compare_airtable_time(hash, time):
  ts1 = time
  ts2 = df_airtable_for_langdeck.loc[df_airtable_for_langdeck["hash"]==hash]["unixtime"].values[0]
  return ts1 > ts2

def get_df_airtable_for_langdeck():

  df_airtable_for_langdeck = pd.melt(df_temp, id_vars=["uid", "lemma","rank","vocabulary_unit","headword","pivot","pivot_en","alphabet","timestamp","theme"], 
                    var_name="language", value_name="translation")

  df_airtable_for_langdeck['hash'] = df_airtable_for_langdeck[['uid', 'language']].sum(axis=1).astype(str)
  #Fonction lambda pour récupérer le nom long de la langue (https://stackoverflow.com/questions/2191699/find-an-element-in-a-list-of-tuples)
  df_airtable_for_langdeck["language_fulltext"] = df_airtable_for_langdeck["language"].apply(lambda x:[item for item in [(a_dict["trigramme"],a_dict["language"]) for a_dict in vk_languages] if item[0] == x][0][1])  
  df_airtable_for_langdeck['timestamp'] = df_airtable_for_langdeck['timestamp'].astype('datetime64[s]')
  df_airtable_for_langdeck['unixtime'] = df_airtable_for_langdeck['timestamp'].astype('datetime64[s]').astype('int')
  # ajout des thématiques  
  # on éclate la liste des thèmes en 3 colonnes
  df_airtable_for_langdeck[["theme_1","theme_2","theme_3"]] = pd.DataFrame([pd.Series(x) for x in df_airtable_for_langdeck.theme])
  return df_airtable_for_langdeck   

def is_audio(f):
  path = "/content/drive/My Drive/appsheet/data/langdeck-1629444/"
  f = path + f
  result = True
  try:
    file = open(f, "r")
  except FileNotFoundError as e:
    # do nothing
    result = False
  return result     

"""
def get_df_langdeck(url):
  # Ouverture de la Sheet Langdeck (mots à traduire)
  sheet = url
  wb  = gc.open_by_url(sheet)
  t_corpus = wb.worksheet("t_corpus_all_entries")
  data_t_corpus = t_corpus.get_all_values()
  df_langdeck = pd.DataFrame(data_t_corpus[1:], columns=data_t_corpus[0]).drop_duplicates()  

  df_langdeck['uid'] = df_langdeck['hash'].str[:8]
  df_langdeck['ordre'] = pd.to_numeric(df_langdeck['ordre'])
  df_langdeck['timestamp'] = df_langdeck['timestamp'].astype('datetime64[s]')
  df_langdeck['unixtime'] = df_langdeck['timestamp'].astype('datetime64[s]').astype('int')
  df_langdeck["translation_pivot_fr"] = df_langdeck["hash"].apply(lambda x:get_translation_pivot_fr(x))
  df_langdeck["translation_pivot_en"] = df_langdeck["hash"].apply(lambda x:get_translation_pivot_en(x))  
  #df_langdeck[["translation_pivot_fr","translation_pivot_en"]] = df_langdeck["hash"].apply(lambda x:get_translation_pivot (x))
  
  df_langdeck['is_audio'] = df_langdeck['audio'].apply(lambda x:is_audio(x))  
  return df_langdeck
"""

def get_translation_pivot_fr (hash):
  r = ""
  try:
    r = df_airtable_for_langdeck.loc[df_airtable_for_langdeck["hash"]==hash]["pivot"].values.item()
  except Exception as e:
    #print (e)
    pass
  return r  

def get_translation_pivot_en (hash):
  r = ""
  try:
    r = df_airtable_for_langdeck.loc[df_airtable_for_langdeck["hash"]==hash]["pivot_en"].values.item()
  except Exception as e:
    #print (e)
    pass
  return r   

def get_translation_pivot (hash):
  piv_fr = ""
  piv_en = ""
  try:
    idx = df_airtable_for_langdeck.loc[df_airtable_for_langdeck["hash"]==hash].index
    piv_fr = df_airtable_for_langdeck.iloc[idx]["pivot"].values.item()
    piv_en = df_airtable_for_langdeck.iloc[idx]["pivot_en"].values.item()
  except Exception as e:
    print (hash)
    pass
  return piv_fr, piv_en

def get_wikipedia(text, language):
  wikipedia = MediaWiki(lang=language)  
  r = ""
  try:
    p = wikipedia.page(text)
    r = p.title + "\n" + p.summary
  except Exception as e:
    r = str(e)
    pass
  return r  


def get_larousse_lst (text):
  from larousse_api import larousse
  d = larousse.get_definitions(text)
  return d

def get_larousse (text):
  from larousse_api import larousse
  vk = larousse.get_definitions(text)  
  s = ""
  for i in vk:
    s += i
  if s == "":
    s = "à renseigner"
  return s

"""# Corpus

## Corpus expressions
"""

#---- Base GSheet
api_key_gmail = "keysgpJWq3aWwqSZe"
db_langdeck_url = "https://docs.google.com/spreadsheets/d/1iJLt3qAKq576eNirMwcsdRltg7nerwa9N70dA0vraa8/" #db-langdeck
sheet = db_langdeck_url
wb  = gc.open_by_url(sheet)

#---- Liste des langues
vk_languages = get_list_languages()

#---- Base Airtable
api_key_airtable = "keyrI98TIqu6mbFcf" 
headers = {"Authorization": "Bearer " + api_key_airtable,"Content-Type" : "application/json"}
base_id = "appawT3gg7cJhPYv0" # Base Nouvelle HV
table_name = 'Corpus'
airtable = Airtable(base_id, table_name, api_key_airtable)

# Chargement table corpus en Dataframe
df_vkat = get_airt_corpus()
#---- maj des uid manquants
check_missing_uids()
# on recharge avec les uids renseignés 
df_airt_corpus = get_airt_corpus()
#---- traduction fr->en manquantes (pivot)
df_airt_corpus["pivot_en"]=df_airt_corpus[["vocabulary_unit","pivot_en"]].apply(lambda x:set_airt_pivot_en(x[0]) if pd.isna(x[1]) else x[1], axis=1)
#---- pivot (fr) manquant
df_airt_corpus["pivot"]=df_airt_corpus[["vocabulary_unit","pivot"]].apply(lambda x:set_airt_pivot_fr(x[0]) if pd.isna(x[1]) else x[1], axis=1)
#======================================================= 
#---- Transposition des colonnes de langues en lignes
#=======================================================
df_airt_corpus = pd.melt(df_airt_corpus,
                         id_vars=["uid", "lemma","rank","vocabulary_unit","headword","pivot","pivot_en","alphabet","timestamp","theme","pending"],
                         var_name="language",
                         value_name="translation")

df_airt_corpus["hash"] = df_airt_corpus[['uid', 'language']].apply(lambda x: "{}{}".format(x[0],x[1]), axis=1)
df_airt_corpus["language_fulltext"] = df_airt_corpus["language"].apply(lambda x:[item for item in [(a_dict["trigramme"],
                                                                                                    a_dict["language"]) for a_dict in vk_languages] if item[0] == x][0][1])  
df_airt_corpus['timestamp'] = df_airt_corpus['timestamp'].astype('datetime64[s]')
df_airt_corpus['unixtime'] = df_airt_corpus['timestamp'].astype('datetime64[s]').astype('int')
# ajout des thématiques  
# on éclate la liste des thèmes en 3 colonnes
df_airt_corpus[["theme_1","theme_2","theme_3"]] = pd.DataFrame([pd.Series(x) for x in df_airt_corpus.theme])
# Vérification qu'il n'existe pas des doublons (hash)
df_airt_corpus[df_airt_corpus['hash'].duplicated()].sort_values("hash")
# on ajoute une colonne de translation state (0=VOID; 1=PENDING; 2=OK; 3=KO)
df_airt_corpus["translation_state"] = df_airt_corpus["translation"].apply(lambda x:0 if pd.isna(x) else 1)
df_airt_corpus.rename(columns={"pivot":"translation_pivot_fr",
                               "pivot_en":"translation_pivot_en",
                               "lemma":"maitre",
                               "rank":"ordre",
                               "vocabulary_unit":"expression",
                               "alphabet":"idx"}, inplace=True)
#---- on charge la base corpus de langdeck (Gsheet)
df_lgdk_corpus = get_lgdk_corpus()
df_lgdk_corpus.drop (["translation_pivot_fr","translation_pivot_en"], axis=1, inplace=True)
df_lgdk_corpus = pd.merge(df_lgdk_corpus, df_airt_corpus[["hash","translation_pivot_fr","translation_pivot_en","headword","pending","theme"]], on="hash")
# suppression des enregistrements dans GSheet et supprimés dans AT
df_lgdk_corpus.drop (
    df_lgdk_corpus[~df_lgdk_corpus["uid"].isin(df_airt_corpus["uid"])].index,
    inplace=True)

# DF de la différence entre AT et GS
df_new_entries = df_airt_corpus[~df_airt_corpus["hash"].isin(df_lgdk_corpus["hash"])]
C = list(df_new_entries.columns.values)
# on ajoute chaque enregistrement en passant par un DICT (pour mieux contrôler)
for index, row in df_new_entries.iterrows():
  d = {}
  for c in C:
    d[c]=row[c]
  d["timestamp"]=str(pd.to_datetime(row['timestamp']))
  d["unixtime"]=d["timestamp"]
  d["audio"]="Files_Files_/Corpus/t_expo_audio/" + row['uid'] + "-" + row['language'] + ".mp3"
  d["is_audio"]=False
  d["uploaded_file"]=""
  if pd.isna(row['theme_1']):
    row['theme_1'] = ""
  if pd.isna(row['theme_2']):
    row['theme_2'] = ""
  if pd.isna(row['theme_3']):
    row['theme_3'] = ""    
  d["theme_1"]=row['theme_1']
  d["theme_2"]=row['theme_2']
  d["theme_3"]=row['theme_3']    
  d["theme_1_svg"]=""
  d["theme_2_svg"]=""
  d["theme_3_svg"]=""
  d["unixtime"]=str(pd.to_datetime(row['timestamp']))
  d["translation_state"]=0
  d["translation_pivot"]=row["translation_pivot_en"]
  d["translation_ai_source"]=""     
  d["translation_score"]=0
  d["translation_state_txt"]=""
  # ajout dans le DF
  df_lgdk_corpus = df_lgdk_corpus.append(d, ignore_index = True)


# mise à jour dans GSheet des données de référentiel (nouvelle méthode + rapide)
# df_lgdk_corpus[["theme_1","theme_2","theme_3","maitre","ordre","expression","idx","translation_pivot_fr","translation_pivot_en"]] = df_lgdk_corpus["hash"].apply(lambda x:update_lgdk_corpus(x))
df_lgdk_corpus.drop(["theme_1","theme_2","theme_3","maitre","ordre","expression","idx","translation_pivot_fr","translation_pivot_en"], axis=1, inplace=True)
df_lgdk_corpus = pd.merge(df_lgdk_corpus, df_airt_corpus[["hash","theme_1","theme_2","theme_3","maitre","ordre","expression","idx","translation_pivot_fr","translation_pivot_en"]], on="hash", how="left")
df_lgdk_corpus.drop(["headword","theme"], axis=1, inplace=True)
df_lgdk_corpus["translation_pivot"]=df_lgdk_corpus["translation_pivot_en"]

df_lgdk_corpus_2 = df_lgdk_corpus.copy()
df_lgdk_corpus.drop(["pending"], axis=1, inplace=True)
# Save to GSheet
save_df_to_gsheet ("t_corpus_all_entries", df_lgdk_corpus)

"""## Corpus lemmes
Indexé sur le Lemme
"""

def get_airt_lemma():
  """ 
  ------------------------------------------
  Récupère un Dataframe du Corpus (vue Lemme) Airtable
  Parameters : aucun

  Returns : aucun
 
  usage : get_airt_lemma():
  ------------------------------------------
  """ 
  vk = airtable.get_all(view='Lemme',sort=['alphabet', 'rank','vocabulary_unit'])
  df_vk = pd.DataFrame.from_records((r['fields'] for r in vk))
  df_vk.drop_duplicates(subset=['vocabulary_unit'], inplace=True)
  return df_vk

# table Corpus Airtable vue Lemme
df_airt_lemma = get_airt_lemma()
### on charge la table t_corpus_head
df_corpus_head = load_df_from_gsheet ("t_corpus_head")
df_corpus_head.drop_duplicates(subset=['terme_pref'], inplace=True) 
# Double DIFF
# on vire ce qui n'est plus dans Airtable
df_corpus_head.drop(df_corpus_head[~df_corpus_head["terme_pref"].isin(df_lgdk_corpus["maitre"])].index, inplace=True) 
# on vire ce qui n'est plus dans GSheet
df_head_lines = df_airt_lemma[(df_airt_lemma["vocabulary_unit"].isin(df_corpus_head["terme_pref"])) & (df_airt_lemma["headword"]==True)]

def f_content_head(lemma):
  df_f = df_airt_lemma.loc[df_airt_lemma["lemma"]==lemma].sort_values(by=['rank'])
  definition = df_corpus_head.loc[df_corpus_head["terme_pref"]==lemma]["definition"].drop_duplicates().values.item()
  if len(definition) == 0:
    print("Appel Larousse " + lemma)
    definition = get_larousse(lemma)
    if len(definition) == 0:
      definition = "à renseigner"
  t = ""
  for index, row in df_f.iterrows():
    strng=""
    for i in row['category']:
      strng +=str(i)    
    if row['rank'] == 1:
      t = t + "<b>"  + row["vocabulary_unit"] + "</b><br />"
      t = t + "<i>" +strng + "</i>&#9;"
      t = t + definition + "<br />"
      t += "<br />"
    elif row['rank'] == 2:
      t = t + "&#9670;&nbsp;"  + row["vocabulary_unit"] + "<br />"
    else:
      t = t + "&#9671;&nbsp;"  + row["vocabulary_unit"] + "<br />"  
  return t

def f_update_head(lemma):
  print(lemma)
  r1 = df_head_lines.loc[df_head_lines["vocabulary_unit"]==lemma]["uid"].values.item()
  r2 = df_head_lines.loc[df_head_lines["vocabulary_unit"]==lemma]["pivot_en"].values.item()
  r3 = f_content_head(lemma)

  return pd.Series([r1, r2, r3])

"""### Ajout des nouveaux lemmes"""

# DF des enregistrements dans AT et pas dans GS
df_head_newlines = df_airt_lemma[(~df_airt_lemma["vocabulary_unit"].isin(df_corpus_head["terme_pref"])) & (df_airt_lemma["headword"]==True)]
# on ajoute chaque enregistrement en passant par un DICT (pour mieux contrôler)
for index, row in df_head_newlines.iterrows():
  d = {}
  d["uid"]=row['uid']
  d["terme_pref"]=row["lemma"]
  d["idx"]=row["alphabet"]
  d["translation_pivot"]=row["pivot"]  
  d["translation_pivot_en"]=row["pivot_en"]    
  d["wikipedia_fr"]=""
  d["wikipedia_en"]=""
  d["wiki_search_text_fr"]=""
  d["wiki_search_text_en"]=""
  d["wikipedia_fr_state"]=0
  d["wikipedia_en_state"]=0 
  d["definition"] = get_larousse(row["lemma"])
  d["content"]= ""
  # ajout dans le DF
  df_corpus_head = df_corpus_head.append(d, ignore_index = True)

# apprès ajout, il faut recharger ce DF
df_head_lines = df_airt_lemma[(df_airt_lemma["vocabulary_unit"].isin(df_corpus_head["terme_pref"])) & (df_airt_lemma["headword"]==True)]  
df_corpus_head["definition"] = df_corpus_head[["terme_pref","definition"]].apply(lambda x:get_larousse(x[0]) if x[1]=="" else x[1], axis=1)
# #### Mide à jour des lignes existantes Head
df_corpus_head[["uid","translation_pivot_en","content"]] = df_corpus_head["terme_pref"].apply(lambda x:f_update_head(x))
# on sauve dans GSheet
save_df_to_gsheet ("t_corpus_head", df_corpus_head)

"""## Corpus : Traductions Google"""

df_corpus_nlp = load_df_from_gsheet ("t_corpus_nlp")
df_corpus_nlp = df_corpus_nlp[["uid","hash","maitre","expression","language","translation","translation_pivot","tr_pivot_lang","translation_state","translation_ai_source","translation_ai"]]	
df_langdeck_ai_temp = df_lgdk_corpus_2.copy()
FILTER_PIVOT = "en"
df_langdeck_ai_temp["translation_pivot"]=df_langdeck_ai_temp[["translation_pivot_fr","translation_pivot_en"]].apply(lambda x:x[0] if FILTER_PIVOT=="fr" else x[1] , axis=1)

# on fait une passe par langue et par langue pivot
# liste complète : 
vk_filter = list(map(lambda d: d["trigramme"], vk_languages))

# concaténation des dataframes
frames =[]
frames.append(df_corpus_nlp)
vk_languages = get_list_languages() 

for FILTER_LANG in vk_filter:
  for dic in vk_languages:
    language = dic["trigramme"]
    iso_code = dic["iso-code"]
    # Si la langue est celle du filtre ?
    if language == FILTER_LANG:
      # Pour chaque code vendor qui gère la langue 
      for vc in dic["vendor-code"]:
        print ("Appel call_nlp_language_vendor " + FILTER_LANG + " " + vc)
        df_langdeck_ai_temp = df_lgdk_corpus_2.copy()
        df_langdeck_ai_temp["pkid"] = df_langdeck_ai_temp["hash"].map(str) + vc
        df_langdeck_ai_temp["translation_pivot"]=df_langdeck_ai_temp[["translation_pivot_fr","translation_pivot_en"]].apply(lambda x:x[0] if FILTER_PIVOT=="fr" else x[1] , axis=1)  
        df_langdeck_ai_temp = df_langdeck_ai_temp [["pkid","hash","maitre","expression","pending","language","translation","translation_pivot","translation_state","translation_ai_source"]]
        df_langdeck_ai_temp["translation_ai"]=""
        df_langdeck_ai_temp["translation_state"]=2
        df_langdeck_ai_temp["tr_pivot_lang"]=FILTER_PIVOT
        df_langdeck_ai_temp["uid"]=df_langdeck_ai_temp["hash"].map(lambda x:x[:8])
        # les champs vides sont mis à NAN
        df_langdeck_ai_temp["translation_ai_source"] = df_langdeck_ai_temp["translation_ai_source"].map(lambda x:np.nan if x=="" else x)
        # enreg présents dans nlp sans traduction => on supprime
        df_corpus_nlp = df_corpus_nlp.loc[~pd.isna(df_corpus_nlp["translation_ai"])]
        # liste des termes à traduire
        lists=[]
        # enreg présents dans corpus et absents dans nlp
        df_trad_nlp = df_langdeck_ai_temp.loc[(~df_langdeck_ai_temp["pkid"].isin(df_corpus_nlp["uid"])) & (df_langdeck_ai_temp["language"]==language)]
        lists.append(df_trad_nlp)
        # enreg présents dans corpus avec status pending
        df_trad_nlp = df_langdeck_ai_temp.loc[(df_langdeck_ai_temp["pending"] == "TRUE") & (df_langdeck_ai_temp["language"]==language)]
        lists.append(df_trad_nlp)
        df_trad_nlp = pd.concat(lists)
        # on supprime les doublons
        df_trad_nlp.drop_duplicates(subset="hash", inplace=True)
        print (f"{language} {iso_code} {vc} {FILTER_PIVOT}")
        df_trad_nlp[["translation_ai","translation_state","translation_ai_source"]] = df_trad_nlp[["translation_pivot","language","translation_state"]].apply(
                lambda x:get_google_translation(x[0],x[1],language, iso_code, vc, FILTER_PIVOT) if int(vc)==12 else get_deepl_translation(x[0],x[1],language, iso_code, vc, FILTER_PIVOT), 
                axis=1, 
                result_type="expand")        
        #df_return = call_nlp_language_vendor (FILTER_LANG, iso_code, vc, FILTER_PIVOT)
        frames.append(df_trad_nlp)
        #frames.append(df_return)

# quand tout a été itéré, on concatène    
df_corpus_trad_ai = pd.concat(frames)
# on uniformise le type
df_corpus_trad_ai['translation_ai_source'] = df_corpus_trad_ai['translation_ai_source'].astype(str)
df_corpus_trad_ai.drop(["pkid","pending"], axis=1, inplace=True)
# on reconstruit l'uid
df_corpus_trad_ai["uid"] = df_corpus_trad_ai["hash"]+df_corpus_trad_ai["translation_ai_source"]
# on supprime les doublons
df_corpus_trad_ai = df_corpus_trad_ai.drop_duplicates(subset=['uid'], keep='last')
# on supprime les codes ai == 0
df_corpus_trad_ai = df_corpus_trad_ai.loc[df_corpus_trad_ai["translation_ai_source"]!="0"]
# par défaut état = 0
df_corpus_trad_ai["translation_state"] = df_corpus_trad_ai["translation_state"].map(lambda x:x if int(x)!=0 else 0)
# on enregistre dans GSheet
save_df_to_gsheet ("t_corpus_nlp", df_corpus_trad_ai)

# on positionne cette variable si une traduction pat l'AI est disponible
# ancienne méthode (lente) df_lgdk_corpus["translation_ai_source"]=df_lgdk_corpus["hash"].apply(lambda x: len(df_corpus_trad_ai.loc[df_corpus_trad_ai["hash"]==x]))
grouped = df_corpus_trad_ai.groupby("hash").count().reset_index()
grouped = grouped[["hash", "uid"]]
grouped.rename ({"uid":"translation_ai_source"}, axis=1, inplace=True)
df_lgdk_corpus.drop (["translation_ai_source"], axis=1, inplace=True)
df_lgdk_corpus = pd.merge(df_lgdk_corpus, grouped, on="hash", how="left")
# si non traduit (NAN) ==> 0
df_lgdk_corpus["translation_ai_source"] = df_lgdk_corpus["translation_ai_source"].map(lambda x: 0 if pd.isna(x) else x )
df_lgdk_corpus["translation_ai_source"] = df_lgdk_corpus["translation_ai_source"].astype(int)

# on enregistre dans GSheet
save_df_to_gsheet ("t_corpus_all_entries", df_lgdk_corpus)

"""# Lexique Thématique
*Pour cette vue, on récupère le DF Langdeck et on extrait la liste des thèmes (l'ensemble des valeurs dans Thème1, 2, 3)
Ensuite, on ajoute autant de colonnes que d'éléments de la liste
Enfin, on supprime les colonnes Theme1,2,3 et on utilise pd.melt pour transposer les colonnes de thème en valeurs. On suprrme les themes = False*
"""

# on crée une liste des valeurs uniques des 3 colonnes de thèmes
frames = [df_lgdk_corpus["theme_1"],df_lgdk_corpus["theme_2"],df_lgdk_corpus["theme_3"]]
vk_theme = list(set(pd.concat(frames).to_list()))
vk_theme = [x for x in vk_theme if x == x]

# on ajoute les colonnes à partir de la liste et on valorise T/F
df_thema = df_lgdk_corpus[["hash","maitre","ordre","expression","idx","language","language_fulltext","translation","theme_1","theme_2","theme_3"]].copy()
df_thema = pd.concat([df_thema, pd.DataFrame(columns = vk_theme)])
for item in vk_theme:
  df_thema[item] = df_thema[["theme_1","theme_2","theme_3"]].apply(lambda x:True if (x[0]==item or x[1]==item or x[2]==item) else False, axis=1)

df_thema = df_thema.drop(["theme_1","theme_2","theme_3"], axis=1)
df_thema = pd.melt(df_thema, id_vars=["hash","maitre","ordre","expression","idx","language","language_fulltext","translation"], 
                    var_name="thema", value_name="is_thema")
df_thema = df_thema.loc[df_thema["is_thema"]]
save_df_to_gsheet ("t_lex_thema", df_thema)

"""# Etudes de cas

## Ajout d'une EC
"""

df_ec_meta = load_df_from_gsheet ("t_ec_meta")
# On recharge si besoin la table vue Lemme
# Chargement table Airtable
base_id_ec = "appi8taq2HcbmWyLo" # Base Nouvelle HV
table_name_ec = 'Etudes de cas'
table_name_location = 'Lieux'
table_name_service = 'Services'
table_name_interpret = 'Interprètes'
table_name_language = 'Langues'
# table Lieux
airtable_location = Airtable(base_id_ec, table_name_location, api_key_airtable)
vk_location = airtable_location.get_all(view='Grid view',sort=['Lieu'])
df_vk_location = pd.DataFrame.from_records((r['fields'] for r in vk_location))
# table Services
airtable_service = Airtable(base_id_ec, table_name_service, api_key_airtable)
vk_service = airtable_service.get_all(view='Grid view',sort=['Name'])
df_vk_service = pd.DataFrame.from_records((r['fields'] for r in vk_service))
# table Interprètes
airtable_interpret = Airtable(base_id_ec, table_name_interpret, api_key_airtable)
vk_interpret = airtable_interpret.get_all(view='Grid view',sort=['Interprète'])
df_vk_interpret = pd.DataFrame.from_records((r['fields'] for r in vk_interpret))
# table Langues
airtable_language = Airtable(base_id_ec, table_name_language, api_key_airtable)
vk_language = airtable_language.get_all(view='Grid view',sort=['Name'])
df_vk_language = pd.DataFrame.from_records((r['fields'] for r in vk_language))

airtable_ec = Airtable(base_id_ec, table_name_ec, api_key_airtable)
vk_ec = airtable_ec.get_all(view='EC',sort=['uid', 'ec_date'])
df_vk_ec = pd.DataFrame.from_records((r['fields'] for r in vk_ec))
df_vk_ec.drop_duplicates(subset=['ec_name'], inplace=True)
# on filtre
df_vk_ec = df_vk_ec.loc[df_vk_ec["ec_status"]=="Réalisé"]

#df_vk_ec = df_vk_ec.loc[df_vk_ec["uid"]==21]

# on ajoute les colonnes pour le merge avec ec_meta
df_vk_ec["uid"]=df_vk_ec["uid"].astype(int)
df_vk_ec["ID"]=df_vk_ec["uid"].apply(lambda x:"EC"+ "{:02d}".format(x))

# on réserve pour la suite (script CR)
df_ec_script = df_vk_ec[["ec_name", "uid","ec_script"]]
# on ne conserve que les ec absentes dans meta
df_vk_ec = df_vk_ec.loc[~df_vk_ec["ID"].isin(df_ec_meta["ID"])]

# on convertit les listes à 1 élément en string
df_vk_ec["ec_service"] = df_vk_ec["ec_service"].apply(lambda x: ''.join([str(i) for i in x]))
# on complète les infos
df_vk_ec["Thème"] = df_vk_ec["ec_service"].apply(lambda x: airtable_service.get(x)['fields']['Name'])
df_vk_ec["Domaine"] = df_vk_ec["ec_service"].apply(lambda x: airtable_service.get(x)['fields']['Domaine'])
df_vk_ec["Présentation"] = df_vk_ec["ec_service"].apply(lambda x: airtable_service.get(x)['fields']['Resume'])
# interprète
df_vk_ec["ec_interpret"] = df_vk_ec["ec_interpret"].apply(lambda x: ''.join([str(i) for i in x]))
df_vk_ec["Interprète"] = df_vk_ec["ec_interpret"].apply(lambda x: airtable_interpret.get(x)['fields']['Nom complet'])
# lieu (location)
df_vk_ec["ec_location"] = df_vk_ec["ec_location"].apply(lambda x: ''.join([str(i) for i in x]))
df_vk_ec["Lieu"] = df_vk_ec["ec_location"].apply(lambda x: airtable_location.get(x)['fields']['Adresse'])
# Langue
df_vk_ec["ec_langue"] = df_vk_ec["ec_langue"].apply(lambda x: ''.join([str(i) for i in x]))
df_vk_ec["Language"] = df_vk_ec["ec_langue"].apply(lambda x: airtable_language.get(x)['fields']['Name'])
df_vk_ec["Language_code"] = df_vk_ec["ec_langue"].apply(lambda x: airtable_language.get(x)['fields']['Code'])

# on supprime les colonnes devenues inutiles
df_vk_ec.drop(["ec_service","ec_interpret","interpret_nom_complet","service_resume","ec_location","location_adress","ec_langue","ec_domaine","ec_script","Notes","Attachments"], axis=1, inplace=True)

df_vk_ec.rename ({"ec_date":"Date","ec_status":"Statut","ec_name":"Nom","ec_doc_url":"CR","ec_contexte":"Contexte"}, axis=1, inplace=True)

df_vk_ec["Cover"]=df_vk_ec["ID"].apply(lambda x: "Files_Files_/assets/images/" + x + ".png")
df_vk_ec["Audio1"]=pd.NA
df_vk_ec["Audio2"]=pd.NA
df_vk_ec.drop(["uid"], axis=1, inplace=True)

frames=[]
frames.append(df_ec_meta)
frames.append(df_vk_ec)
df_ec_meta_update = pd.concat(frames)

save_df_to_gsheet ("t_ec_meta", df_ec_meta_update)

"""## Ajout des scripts new EC"""

airtable_ec = Airtable(base_id_ec, table_name_ec, api_key_airtable)
vk_ec = airtable_ec.get_all(view='EC',sort=['uid', 'ec_date'])
df_vk_ec = pd.DataFrame.from_records((r['fields'] for r in vk_ec))
df_vk_ec.drop_duplicates(subset=['ec_name'], inplace=True)
# on filtre
df_vk_ec = df_vk_ec.loc[df_vk_ec["ec_status"]=="Réalisé"]
# on ajoute les colonnes pour le merge avec ec_meta
df_vk_ec["uid"]=df_vk_ec["uid"].astype(int)
df_vk_ec["ID"]=df_vk_ec["uid"].apply(lambda x:"EC"+ "{:02d}".format(x))
# on réserve pour la suite (script CR)
df_ec_script = df_vk_ec[["ec_name", "uid","ec_script"]]


table_name_script = 'EC_Script'
# table Lieux
airtable_script = Airtable(base_id_ec, table_name_script, api_key_airtable)
vk_script = airtable_script.get_all(view='Grid view',sort=['sequence','uid'])
df_vk_script = pd.DataFrame.from_records((r['fields'] for r in vk_script))

df_ec_meta = load_df_from_gsheet ("t_ec_meta")
df_ec_script_content = load_df_from_gsheet ("t_ec_content_fr")
df_ec_script_content_new = df_ec_script_content.drop(df_ec_script_content.index)

df_ec_script["EC_Key"] = df_ec_script["uid"].apply(lambda x:"EC"+ "{:02d}".format(x))

# on ne conserve que les ec scripts absentes dans meta ET avec du contenu
df_ec_script = df_ec_script.loc[(~df_ec_script["EC_Key"].isin(df_ec_script_content["EC_Key"])) & (~pd.isna(df_ec_script["ec_script"]))]

if len(df_ec_script) > 0:

  # on explose (transpose) les ids de dialogues
  df_ec_script = df_ec_script.explode('ec_script')

  df_ec_script["PhraseFR"] = df_ec_script["ec_script"].apply(lambda x: airtable_script.get(x)['fields']['texte'])
  df_ec_script["Locuteur"] = df_ec_script["ec_script"].apply(lambda x: airtable_script.get(x)['fields']['locuteur'])
  df_ec_script["seq"] = df_ec_script["ec_script"].apply(lambda x: "{:02d}".format(airtable_script.get(x)['fields']['uid']))
  df_ec_script["ID"] = df_ec_script[["EC_Key","seq"]].apply(lambda x: x[0] + "-FRA-001_" + x[1], axis=1)
  df_ec_script["AudioFR"] = df_ec_script["ID"].apply(lambda x: "Files_Files_/assets/audio/" + x + ".mp3")

  df_ec_script["Sens"] = df_ec_script["Locuteur"].apply(lambda x: "A" if x == "L’orthoptiste" else "B")

  df_ec_script.drop(["uid","ec_name","ec_script","seq"], axis=1, inplace=True)

  frames =[]
  frames.append(df_ec_script_content)
  frames.append(df_ec_script)
  df_ec_script_update = pd.concat(frames)

  save_df_to_gsheet ("t_ec_content_fr", df_ec_script_update)

"""## Vocabulaire EC"""

import hashlib
#---- Chargement table Airtable EC_Content dans un DF
vk_at_ecc = Airtable(base_id, 'EC_Content', api_key_airtable).get_all(view='CR',sort='ID1')
df_vkat_ecc = pd.DataFrame.from_records((r['fields'] for r in vk_at_ecc))
# on transpose les listes de mots
df_vkat_ecc=df_vkat_ecc.explode("Vocabulaire")
# on récupère les valeurs à partir du ID record
df_vkat_ecc["vocabulary_unit"] = df_vkat_ecc["Vocabulaire"].map(lambda x:airtable.get(x)["fields"]["vocabulary_unit"] if not pd.isna(x) else "")
df_vkat_ecc["headword"] = df_vkat_ecc["Vocabulaire"].map(lambda x:airtable.get(x)["fields"]["lemma"] if not pd.isna(x) else "")
# chargement de la table des contenus EC (FR)
df_ec_content_fr = load_df_from_gsheet("t_ec_content_fr")
# onb récupère les données avec la clé (phraseFR)
df_vkat_ecc=pd.merge(df_vkat_ecc, df_ec_content_fr [["PhraseFR","ID","EC_Key"]], left_on="Phrase", right_on="PhraseFR", how="left")

df_vocab = load_df_from_gsheet ("t_ec_vocabulary")
# la clé est double : EC_KeyFR + Vocabulaire - ID + vocabulary_unit
df_vkat_ecc.drop(["Vocabulaire","ID1","ID2","EC","EC_Key","Locuteur","Phrase","PhraseFR"], axis=1, inplace=True)
df_vkat_ecc.rename({"vocabulary_unit":"Vocabulaire","ID":"EC_KeyFR"}, axis=1, inplace=True)
df_vkat_ecc["Vocabulaire"] = df_vkat_ecc["Vocabulaire"].map(lambda x:np.nan if x=="" else x)
df_vkat_ecc = df_vkat_ecc.loc[~pd.isna(df_vkat_ecc["Vocabulaire"])]

# on créé une clé unique
df_vkat_ecc["PKID"]=df_vkat_ecc[["EC_KeyFR","Vocabulaire"]].apply(lambda x:x[0]+"-"+str(int(hashlib.sha1((x[0]+x[1]).encode("utf-8")).hexdigest(), 16) % (10 ** 8)), axis=1)
# on concatène
frames=[]
frames.append(df_vocab)
frames.append(df_vkat_ecc)
df_vocab = pd.concat(frames)
# on vire les doublons
df_vocab.drop_duplicates(subset="PKID", inplace=True)

# on enregiste
save_df_to_gsheet ("t_ec_vocabulary", df_vocab)

"""## Traductions IA pour les EDC

On charge la table t_ec_content_trad et on créé une table des propositions  comme t_corpus_nlp
"""

FILTER_LANG

df_ec_trad = load_df_from_gsheet ("t_ec_content_trad")
df_ec_meta = load_df_from_gsheet ("t_ec_meta")
df_ec_ct_substitute = load_df_from_gsheet ("t_ec_ct_substitute")
df_ec_nlp = load_df_from_gsheet ("t_corpus_nlp")
# on vide le contenu, on conserve le gabarit
df_ec_nlp.drop(df_ec_nlp.index,inplace=True) 

df_ec_fr = load_df_from_gsheet ("t_ec_content_fr")
df_ec_fr["translation_pivot_fr"]=df_ec_fr["PhraseFR"]
df_ec_fr["translation_pivot_en"]=pd.NA
df_ec_fr["translation_pivot"]=df_ec_fr[["translation_pivot_fr","translation_pivot_en"]].apply(lambda x:x[0] if FILTER_PIVOT=="fr" else x[1] , axis=1)
# clé EDC+TRAD
df_ec_fr["id_trad"]=df_ec_fr["ID"].apply(lambda x:x[:5] + FILTER_LANG.upper() + "-" + x[9:])
# nombre de trad IA disponibles pour chaque clé
df_ec_fr["translation_ai_source"]=df_ec_fr["id_trad"].apply(lambda x:len(df_ec_ct_substitute.loc[df_ec_ct_substitute["ID"]==x]))
df_ec_fr["PhraseTR"]=pd.NA
df_ec_fr["AudioTR"]=pd.NA
df_ec_fr["AudioTR_AI"]=pd.NA
df_ec_fr.rename(columns={"ID":"EC_KeyFR","id_trad":"ID"}, inplace=True)

FILTER_PIVOT = "fr"
FILTER_LANG = "alb"

"""#### On a dans A toutes les EC en Fr et dans B les traductions existantes en RUS
On complète les traductions manquantes par appel IA
"""

# DF des enregistrements dans t_ec_content_fr et pas dans t_ec_content_trad
df_newlines = df_ec_fr[~df_ec_fr["ID"].isin(df_ec_trad["ID"])]
df_ko = df_ec_trad.loc[df_ec_trad["translation_ai_source"]=="3"]
df_kolines = df_ec_fr[df_ec_fr["ID"].isin(df_ko["ID"])]
# on ajoute les lignes absentes dans TRD
frames =[]
frames.append(df_ec_trad)
frames.append(df_newlines)
frames.append(df_kolines)
df_ec_trad = pd.concat(frames)

df_ec_fr.tail()

df_ec_fr.iloc[1]

df_ec_trad.iloc[0]

vk_filter = list(map(lambda d: d["trigramme"], vk_languages))

# concaténation des dataframes
frames =[]
frames.append(df_corpus_nlp)
vk_languages = get_list_languages() 

for FILTER_LANG in vk_filter:
  for dic in vk_languages:
    language = dic["trigramme"]
    iso_code = dic["iso-code"]
    # Si la langue est celle du filtre ?
    if language == FILTER_LANG:
      # Pour chaque code vendor qui gère la langue 
      for vc in dic["vendor-code"]:
        # appel de la traduction si absente

'''
df_ec_trad.drop('translation_ai', axis=1, inplace=True)
df_ec_trad = pd.merge(df_ec_trad, df_ec_fr[["ID","translation_ai"]], how="left", on="ID")
'''

def simpleGoogleTranslate (txt, pivot, iso_code):
  from deep_translator import GoogleTranslator
  try:
    r = GoogleTranslator(source=pivot, target=iso_code).translate(txt)
    print (r)
  except Exception as e:
    print (e)
    pass
  return r

# on met à jour les traductions IA
iso_code = df_lang_vendor.loc[df_lang_vendor["language"]==FILTER_LANG]["iso-code"].values.item()
df_ec_trad["EC_Key"]=df_ec_trad["ID"].apply(lambda x:x[:8])
df_ec_trad["translation_ai"] = df_ec_trad[["PhraseFR","translation_ai","EC_Key"]].apply(
        lambda x:simpleGoogleTranslate(x[0], FILTER_PIVOT, iso_code) if ((pd.isna(x[1]) or x[1]=="") and x[2][5:].lower()==FILTER_LANG) else x[1], 
        axis=1)

# si la traduction est absente en PhraseTR => On colle la traduction IA
df_ec_trad["PhraseTR"] = df_ec_trad[["PhraseTR","translation_ai"]].apply(lambda x:x[1] if (pd.isna(x[0]) or x[0]=="") else x[0], axis=1)
f_path = "Files_Files_/assets/audio/"
f_path_ai = "Files_Files_/assets/audio/tts/"
df_ec_trad["translation_pivot"] = df_ec_trad[["translation_pivot","PhraseFR"]].apply(lambda x: x[1] if (pd.isna(x[0]) or x[0]=="") else x[0], axis=1)

'''
df_ec_trad["AudioFR"] = df_ec_trad[["AudioFR","EC_KeyFR"]].apply(lambda x: f_path + x[1] + ".mp3" if (pd.isna(x[0]) or x[0]=="") else x[0], axis=1)
df_ec_trad["AudioTR"] = df_ec_trad[["AudioTR","ID"]].apply(lambda x: f_path + x[1] + ".mp3" if (pd.isna(x[0]) or x[0]=="") else x[0], axis=1)
df_ec_trad["AudioTR_AI"] = df_ec_trad[["AudioTR_AI","ID"]].apply(lambda x: f_path_ai + x[1] + ".mp3" if (pd.isna(x[0]) or x[0]=="") else x[0], axis=1)
'''

df_ec_trad["AudioFR"] = df_ec_trad[["AudioFR","EC_KeyFR"]].apply(lambda x: f_path + x[1] + ".mp3", axis=1)
df_ec_trad["AudioTR"] = df_ec_trad[["AudioTR","ID"]].apply(lambda x: f_path + x[1] + ".mp3" , axis=1)
df_ec_trad["AudioTR_AI"] = df_ec_trad[["AudioTR_AI","ID"]].apply(lambda x: f_path_ai + x[1] + ".mp3", axis=1)

# on enregistre dans GSheet
try:
  t_ = wb.worksheet("t_ec_content_trad")
  wb.del_worksheet(t_)
except:
  print ("Onglet inexistant !")
  pass
wb.add_worksheet("t_ec_content_trad", 1, 1)
export_sheet = wb.worksheet("t_ec_content_trad")
set_with_dataframe(export_sheet, df_ec_trad)

# Il faut mettre à jour la table de correspondance des EC t_ec_bylang
df_ec_index = pd.DataFrame()
df_ec_index["ID"]=df_ec_trad["EC_Key"].drop_duplicates()

# Il faut mettre à jour la table de correspondance des EC t_ec_bylang
df_ec_index = pd.DataFrame()
df_ec_index["ID"]=df_ec_trad["EC_Key"].drop_duplicates()
df_ec_index["EC_Key"]=df_ec_index["ID"].map(lambda x:x[:4])
df_ec_index = pd.merge(df_ec_index, df_ec_meta[["ID","Nom"]], how="left", left_on="EC_Key", right_on="ID")
df_ec_index.rename(columns={"ID_x":"ID","Nom":"EC_Desc"}, inplace=True)
df_ec_index.drop('ID_y', axis=1, inplace=True)
df_ec_index["Language_code"]=df_ec_index["ID"].map(lambda x:x[5:].lower())
df_ec_index["code_2d"]=df_ec_index["Language_code"].map(lambda x: df_lang_vendor.loc[df_lang_vendor["language"]==x]["iso-code"].values.item()) 
df_ec_index["Language"] = df_ec_index["Language_code"].map(lambda x: list(filter(lambda i: i['trigramme'] == x, vk_languages))[0]["language"])
df_ec_index["Interpret"] = pd.NA
df_ec_index = pd.merge(df_ec_index, df_ec_meta[["ID","Cover"]], how="left", left_on="EC_Key", right_on="ID")
df_ec_index.drop('ID_y', axis=1, inplace=True)
df_ec_index.rename(columns={"ID_x":"ID","Nom":"EC_Desc"}, inplace=True)

# on enregistre dans GSheet
try:
  t_corpus = wb.worksheet("t_ec_bylang")
  wb.del_worksheet(t_corpus)
except:
  print ("Onglet inexistant !")
  pass
wb.add_worksheet("t_ec_bylang", 1, 1)
export_sheet = wb.worksheet("t_ec_bylang")
set_with_dataframe(export_sheet, df_ec_index)

"""# TTS"""

import os
!pip install gtts --upgrade
from gtts import gTTS
# !gtts-cli --all

!pip install pyttsx3
!pip install pypiwin32
!sudo apt install espeak
!pip3 install pyaudio
import pyttsx3

def text2speech (text, language, path, slow):
  # https://www.geeksforgeeks.org/convert-text-speech-python/
  # Passing the text and language to the engine, 
  # here we have marked slow=False. Which tells 
  # the module that the converted audio should 
  # have a high speed
  r = True
  if language in ["fa","hy"]:
    return False
  try:
    myobj = gTTS(text=text, lang=language, slow=slow)
    myobj.save(path)
    print (path + "(" + language + ") : okay")
  except Exception as e:
    print (language + " - " + str(e))
    r = False
    try:
      if os.path.getsize(path) == 0:
        os.remove(path)
        print (path + " file removed.")
    except OSError:
      pass    
    
  return r

def text2speech_2 (text, language, path, slow):
  # Initialize the converter
  converter = pyttsx3.init()
  # Can be more than 100
  converter.setProperty('rate', 150)
  # Set volume 0-1
  converter.setProperty('volume', 0.7)
  """
  voices = converter.getProperty('voices')
  for voice in voices:
      # to get the info. about various voices in our PC 
      print("Voice:")
      print("ID: %s" %voice.id)
      print("Name: %s" %voice.name)
      print("Age: %s" %voice.age)
      print("Gender: %s" %voice.gender)
      print("Languages Known: %s" %voice.languages)  
  """
  r = True
  if language in ["fa","hy"]:
    return False
  try:
    # We can use file extension as mp3 and wav, both will work
    converter.save_to_file(text, path)
    print (path + "(" + language + ") : okay")
  except Exception as e:
    print (language + " - " + str(e))
    r = False
    try:
      if os.path.getsize(path) == 0:
        os.remove(path)
        print (path + " file removed.")
    except OSError:
      pass    
    
  return r

voices = converter.getProperty('voices')
for voice in voices:
    # to get the info. about various voices in our PC 
    print("Voice:")
    print("ID: %s" %voice.id)
    print("Name: %s" %voice.name)
    print("Age: %s" %voice.age)
    print("Gender: %s" %voice.gender)
    print("Languages Known: %s" %voice.languages)

"""
voices = converter.getProperty('voices')
index = 0
for voice in voices:
   print(f'index-> {index} -- {voice.name}')
   
text="به کجا صدمه می زنی؟"
path="alb-test.mp3"
converter = pyttsx3.init()
converter.setProperty('rate', 110)
converter.setProperty('volume', 0.7)
converter.setProperty('voice', "persian")
converter.save_to_file(text, path)
#converter.say(text)
#converter.runAndWait()
"""

'''
def text2speech (text, language, path):
  # https://www.geeksforgeeks.org/convert-text-speech-python/
  # Passing the text and language to the engine, 
  # here we have marked slow=False. Which tells 
  # the module that the converted audio should 
  # have a high speed
  r = False
  try:
    myobj = gTTS(text=text, lang=language, slow=False)
    myobj.save(path)
    print (path)
    r = True
  except Exception as e:
    print(str(e))

  return r

'''

vk_filter = list(map(lambda d: d["trigramme"], vk_languages))
"""
for langue in vk_filter:
  iso_code = df_lang_vendor.loc[df_lang_vendor["language"]==langue]["iso-code"].values.item()
  print (langue + ":" + iso_code)
"""

"""### Version langue choisie"""

is_audio('/content/drive/My Drive/appsheet/data/langdeck-1629444/Files_Files_/assets/audio/tts/EC03-PRS-001_01.mp3')

audio_path = "/content/drive/My Drive/appsheet/data/langdeck-1629444/"
filtre = ["alb"]
filtre_ec=["EC14-ALB"]
force = True
slow = True
library="gtts"

for langue in filtre:
  iso_code = df_lang_vendor.loc[df_lang_vendor["language"]==langue]["iso-code"].values.item()
  LANG_FILTER = langue
  df_ec_trad_filtered = df_ec_trad.loc[df_ec_trad["EC_Key"].isin(filtre_ec)]
  df_ec_trad_filtered["language"]=df_ec_trad_filtered["EC_Key"].map(lambda x:x[5:].lower())
  df_ec_trad_filtered = df_ec_trad_filtered.loc[df_ec_trad_filtered["language"]==LANG_FILTER]
  if len(df_ec_trad_filtered) > 0:
    df_ec_trad_filtered[["PhraseFR","EC_Key","AudioFR"]].apply(lambda x:text2speech(x[0], "fr", audio_path + x[2], False) if is_audio(x[2])==False else False, axis=1)
    if library=="gtts":
      df_ec_trad_filtered[["PhraseTR","EC_Key","AudioTR_AI"]].apply(lambda x:text2speech(
          x[0], 
          df_lang_vendor.loc[df_lang_vendor["language"]==x[1][5:].lower()]["iso-code"].values.item(), 
          audio_path + x[2],
          slow) 
          if (is_audio(x[2])==False or force==True) else False, axis=1)   
    else:
      df_ec_trad_filtered[["PhraseTR","EC_Key","AudioTR_AI"]].apply(lambda x:text2speech(
          x[0], 
          df_lang_vendor.loc[df_lang_vendor["language"]==x[1][5:].lower()]["iso-code"].values.item(), 
          audio_path + x[2],
          slow) 
          if (is_audio(x[2])==False or force==True) else False, axis=1)

"""### Version tt langues"""

audio_path = "/content/drive/My Drive/appsheet/data/langdeck-1629444/"
for langue in vk_filter:
  iso_code = df_lang_vendor.loc[df_lang_vendor["language"]==langue]["iso-code"].values.item()
  LANG_FILTER = langue
  df_ec_trad_filtered = df_ec_trad.copy()
  df_ec_trad_filtered["language"]=df_ec_trad_filtered["EC_Key"].map(lambda x:x[5:].lower())
  df_ec_trad_filtered = df_ec_trad_filtered.loc[df_ec_trad_filtered["language"]==LANG_FILTER]
  if len(df_ec_trad_filtered) > 0:
    df_ec_trad_filtered[["PhraseFR","EC_Key","AudioFR"]].apply(lambda x:text2speech(x[0], "fr", audio_path + x[2], False) if is_audio(x[2])==False else False, axis=1)
    df_ec_trad_filtered[["PhraseTR","EC_Key","AudioTR_AI"]].apply(lambda x:text2speech(
        x[0], 
        df_lang_vendor.loc[df_lang_vendor["language"]==x[1][5:].lower()]["iso-code"].values.item(), 
        audio_path + x[2],
        False) 
        if is_audio(x[2])==False else False, axis=1)



"""# T_EC_TRAD SUBSTITUTES

algorithme : on récupère les ID manquants dans t_ec_ct_substitute
"""

tbl = wb.worksheet("t_ec_ct_substitute")
data = tbl.get_all_values()
df_ec_ct_sub = pd.DataFrame(data[1:], columns=data[0])

"""Différence entre dfec_trad et dfd_ec_subst"""

df_x = df_ec_trad[["ID","PhraseTR","translation_ai"]].melt(id_vars="ID")
df_x.drop("variable", axis=1, inplace=True)
df_x.rename ({"value":"translation"}, axis=1, inplace=True)
df_x = df_x.drop_duplicates()
df_x["uid"]=df_x.index.map(str)
df_x["uid"]=df_x[["uid","ID"]].apply(lambda x:x[1]+"-"+x[0].zfill(9), axis=1)
df_x["translator_email"]=pd.NA
df_x["translator_name"]=pd.NA
df_x["translation_date"]=pd.NA
df_x["validator_email"]=pd.NA
df_x["validator_name"]=pd.NA
df_x["evaluation_date"]=pd.NA
df_x["timestamp"]=pd.NA
df_x["evaluation_score"]=pd.NA

# DIFF entre Trad et Subst
df_diff = pd.concat([df_ec_ct_sub,df_x]).drop_duplicates(subset="ID", keep=False)
# on concatène la différence avec l'existant
df_total = pd.concat([df_ec_ct_sub,df_diff])
# subset des doublons (>1)
df_dup = df_total[df_total.duplicated("ID", keep=False)]
# DIFF entre dup et total
df_diff = pd.concat([df_dup,df_total]).drop_duplicates(subset="ID", keep=False)

df_diff["audio"]=df_diff["ID"].map(lambda x: "Files_Files_/assets/audio/tts/" + x + ".mp3")
df_diff["uid"]=df_diff["ID"].map(lambda x:x)

# subset des doublons FIRST et LAST
pd.options.mode.chained_assignment = None  # default='warn'
df_dup1 = df_dup.drop_duplicates(subset="ID", keep="first")
df_dup2 = df_dup.drop_duplicates(subset="ID", keep="last")
# doublons en trop (>2)
# df_dup.loc[(~df_dup["uid"].isin(df_dup1["uid"])) & (~df_dup["uid"].isin(df_dup2["uid"]))]
df_dup1["audio"]=df_dup1["ID"].map(lambda x: "Files_Files_/assets/audio/tts/" + x + ".mp3")
df_dup1["uid"]=df_dup1["ID"].map(lambda x:x)
df_dup2["uid"]=df_dup2["ID"].map(lambda x:x+"-2")
df_dup2["audio"]=df_dup2["uid"].map(lambda x: "Files_Files_/assets/audio/tts/" + x + ".mp3")

df_dups = pd.concat([df_dup1, df_dup2])
df_final = pd.concat([df_dups, df_diff])

len(df_final)

"""# traitement des audios (TTS)"""

audio_path = "/content/drive/My Drive/appsheet/data/langdeck-1629444/"
vk_filter = list(map(lambda d: d["trigramme"], vk_languages))
filtre = ["alb"]
force = False
slow = False

for langue in filtre:
  iso_code = df_lang_vendor.loc[df_lang_vendor["language"]==langue]["iso-code"].values.item()
  LANG_FILTER = langue
  df_final_tts = df_final.copy()
  df_final_tts["language"]=df_final_tts["ID"].map(lambda x:x[5:8].lower())
  df_final_tts = df_final_tts.loc[df_final_tts["language"]==LANG_FILTER]
  if len(df_final_tts) > 0:
    df_final_tts[["translation","language","audio"]].apply(lambda x:text2speech(
        x[0], 
        df_lang_vendor.loc[df_lang_vendor["language"]==x[1]]["iso-code"].values.item(), 
        audio_path + x[2],
        slow) if (is_audio(x[2])==False or force==True) else False, axis=1)

"""
df_final["new_proposal"]=pd.NA
df_final["verbatim"]=pd.NA
"""

# on enregistre dans GSheet
try:
  t_ = wb.worksheet("t_ec_ct_substitute")
  wb.del_worksheet(t_)
except:
  print ("Onglet inexistant !")
  pass
wb.add_worksheet("t_ec_ct_substitute", 1, 1)
export_sheet = wb.worksheet("t_ec_ct_substitute")
set_with_dataframe(export_sheet, df_final)

"""TCORPUS MINI"""

tbl = wb.worksheet("t_corp_mini")
data = tbl.get_all_values()
df_corp_mini = pd.DataFrame(data[1:], columns=data[0])

audio_path = "/content/drive/My Drive/appsheet/data/langdeck-1629444/Files_Files_/assets/audio/tts/top10/"
df_corp_mini["audio"]=df_corp_mini["id"].map(lambda x:audio_path + x + ".mp3")

is_audio("Files_Files_/assets/audio/tts/top10/alb-1.mp3")

df_corp_mini.loc[df_corp_mini["wals_code"]=="alb"]

iso_code = df_lang_vendor.loc[df_lang_vendor["language"]==langue]["iso-code"].values.item()
df_corp_mini.loc[df_corp_mini["wals_code"]=="alb"][["exp","wals_code","audio"]].apply(lambda x:text2speech(x[0], "sq", x[2], False) if is_audio(x[2])==False else False, axis=1)

"""# Intégration des EC Raw pour Airtable (Wrangling)"""

# récupération du fichier txt
# load doc into memory
def load_doc(filename):
  # open the file as read only
  file = open(filename, mode='rt', encoding='utf-8')
  # read all text
  text = file.read()
  # close the file
  file.close()
  return text

# save doc to gdrive
def save_doc(filename, content):
  with open(filename, 'w') as f:
    for line in content:
      f.write("%s\n" % line)    

# split a loaded document into sentences
def to_sentences(doc):
  return doc.strip().split('\n')  

# clean lines
def clean_lines(lines):
  cleaned = list()
  # prepare regex for char filtering
  re_print = re.compile('[^%s]' % re.escape(string.printable))  
  # prepare translation table for removing punctuation
  table = str.maketrans('', '', string.punctuation)
  for line in lines:
    # normalize unicode characters
    #line = unicodedata.normalize('NFD', line).encode('ascii', 'ignore')
    #line = line.decode('UTF-8')
    # tokenize on white space
    line = line.split()
    # convert to lower case
    line = [word.lower() for word in line]
    # remove punctuation from each token
    line = [word.translate(table) for word in line]
    # remove non-printable chars form each token
    line = [re_print.sub('', w) for w in line]
    # remove tokens with numbers in them
    line = [word for word in line if word.isalpha()]
    # store as string
    cleaned.append(' '.join(line))
  return cleaned

# simple nettoyage pour intégration dans Airtable
def clean_lines_lite(lines):
  cleaned = list()
  # prepare regex for char filtering
  re_print = re.compile('[^%s]' % re.escape(string.printable))  
  # prepare translation table for removing punctuation
  table = str.maketrans('', '', string.punctuation)
  for line in lines:
    # suppression du point en fin de phrase
    line = line.rstrip().rstrip(".")
    # suppression du * en début de phrase
    line = line.lstrip("\*")    
    # supression des espaces en trop
    line = ' '.join(line.split())
    # store as string
    cleaned.append(line.capitalize())
  return cleaned

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Trad-Union/Corpus/ASAMLA/resources/case_studies/
ec = "EC04"
filename = ec+".txt"

doc = load_doc(filename)
sentences = to_sentences(doc)
# filtre sur les dialogues (commencent par *)
sentences = [s for s in sentences if s.startswith("*")]
cleanf=clean_lines_lite(sentences)
filename = ec+"-CLEANED.txt"
save_doc(filename, cleanf)
subset=[]
subset.append(cleanf)
cleanf=clean_lines(sentences)
subset.append(cleanf)

subset

"""# Lemmatization et Tokeniszation"""

# on passe en DataFrame
df_raw = pd.DataFrame(subset).T
df_raw.set_axis(["fr","fr_raw"], axis=1, inplace=True)

# tokenisation

df_raw

nlp = spacy.load("fr_core_news_sm")
nlp.add_pipe('french_lemmatizer', name='lefff')

def get_lemma(s):
  doc = nlp(s)
  vk=[]
  for d in doc:
    vk.append(d.lemma_)
  return vk

"""# Détection de langues"""

# Polyglot
!pip3 install pyicu
!pip3 install pycld2
!pip install polyglot
!sudo apt-get install python-numpy libicu-dev
from polyglot.detect import Detector

# HuggingFace
#====== Deep Translate ================
!pip install transformers
!pip install txtai[pipeline] --upgrade
#!pip install sacremoses
#from txtai.pipeline import Translation
#translate = Translation()

!pip install -U deep-translator
from transformers import pipeline
classifier = pipeline("text-classification", model="papluca/xlm-roberta-base-language-detection")

"""## en->ti"""

#https://huggingface.co/Helsinki-NLP/opus-mt-en-ti?text=My+name+is+Sarah+and+I+live+in+London
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
tokenizer = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-ti")
model = AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-mt-en-ti")

translator  = pipeline("translation_en_to_ti", model=model, tokenizer=tokenizer)

text_en ="Liver"

translator(text_en)

text_ti="ጸላም ከብዲ"

"""## ti->en"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
tokenizer = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-ti-en")
model = AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-mt-ti-en")

translator  = pipeline("translation_ti_to_en", model=model, tokenizer=tokenizer)

translator(text_ti)

for language in Detector(text).languages:
  print(language)

classifier(text)

from deep_translator import GoogleTranslator
input="አብ ቐረባ እዋን አንቲቢዮቲክ በሊዕኻ'ዶ ኔርካ ?"
translation = GoogleTranslator(source='am', target="en").translate(input)
translation

from google.cloud import translate

def get_supported_languages(project_id="YOUR_PROJECT_ID"):
    """Getting a list of supported language codes."""

    client = translate.TranslationServiceClient()

    parent = f"projects/{project_id}"

    # Supported language codes: https://cloud.google.com/translate/docs/languages
    response = client.get_supported_languages(parent=parent)

    # List language codes of supported languages.
    print("Supported Languages:")
    for language in response.languages:
        print("Language Code: {}".format(language.language_code))



import spacy
# Construction from subclass
from spacy.lang.fr import French
nlp = French()
url = 'https://lactualite.com/sante-et-science/stress-intense-coeur-vulnerable/'
r = requests.get(url)
html = r.text
from bs4 import BeautifulSoup
soup = BeautifulSoup(html, 'html5lib')
text = soup.get_text()
gutten_nlp = nlp(text[:99999])
rep = []

for token in gutten_nlp:
    rep.append((token))
    rep.append((token.orth_))
    rep.append((token.orth))

print(rep)

wikipedia = MediaWiki(lang="en")  
text="Imaging"
r = ""
try:
  p = wikipedia.page(text)
  r = p.title + "\n" + p.summary
except Exception as e:
  r = str(e)
  pass

r

from transformers import pipeline
# https://blog.huawei.com/2022/02/01/speaking-your-language-transformer-machine-translation/
# https://towardsdatascience.com/keyword-extraction-with-bert-724efca412ea

from deep_translator import GoogleTranslator

s="""Bonjour, comment ça va ? Ton nom c’est Hussein ?
Tu veux bien que je mette ça à un de tes poignets ?
L'enfant
Oui

C’est bien pour toi ?
Je vais faire une prise de sang. Tu en as déjà eu ?
On prélève un petit peu le sang pour le faire analyser.
Y a-t-il un bras que tu préfères ?
Quel côté plus que l’autre préfères-tu ?
Je mets un élastique pour faire gonfler ta veine pour qu’on la voit mieux. Ça doit serrer mais pas te faire mal.
Ça va être froid.
C’est une toute petite aiguille que j'utilise pour les bébés d’habitude.
Tu peux compter jusqu’à 10.
Est-ce que ça te va ?
L'enfant
Oui.

C’est presque fini. Ça met des fourmis dans ton bras ?
C’est normal.
On a l’impression que je prends beaucoup de sang mais il n’y en a pas beaucoup ; en fait il n’y a pas grand chose.
On fait un bilan complet, ça permet de le suivre au niveau de son bilan. Tu peux appuyer sur le petit coton ?
Le plus dur est fait. Après ça va être facile.
Suis-tu un régime particulier ?
L'enfant
Je mange tout.

Café ? Chocolat ? Jus d’orange ? Pain beurre confiture ?
Je vais prendre ta tension.
Ça va serrer fort le bras au début t’inquiète pas c’est normal.
Est-ce qu’on t’a montré la gélule que tu vas avaler cet après-midi ?
Je te la ferai avaler avec un verre d’eau, il ne faudra pas la cracher mais la vallée avec ce verre d’eau.
Après je te donne une compote, et tu auras aussi un petit goûter.
C’est important qu’il ne mange pas avant 14 heures ! Pour que l’estomac soit vide, parce qu’il est feignant et si il y a autre chose à digérer, ça ne va pas être efficace.
Pour le reste de l’hospitalisation, il va être hospitalisé jusqu’à mercredi.
Est-ce qu’il a tendance à être constipé ?

Non.

C’est important qu’il ne garde pas le médicament dans les intestins.
L’iode et le carburant de la glande compta enlever entre parenthèses thyroïde. Le but c’est qu'ils se fixent sur ce qu’il reste de la thyroïde pour le détruire.
Il n’y a pas d’effets indésirables ou très peu. Un peu de chaleur au niveau du cou, comme une angine.
Tout ce qui ne va pas être fixé va être éliminé le plus vite possible c’est pour cela qu’il est hospitalisé.
Il faut boire de l’eau pour les reins, aller aux toilettes.
Et garder une certaine distance entre parenthèses 1 m.
Vendredi scintigraphie, c’est une image, une photo pour voir où Lyon est allé.
Il n’y a pas d’injection pour cet examen.
Avez-vous besoin d’un bon de transport ?
Oui.

Il ne faut pas prendre les transports en commun sinon il faut être éloigné des enfants et des femmes enceintes.
Le jour de la scintigraphie, il faudra reprendre les médicaments si noel lis vos thyroxin.
Quand est le rendez-vous avec le docteur C. ?

Il y a une prise de sang à faire dans un mois et demi.

Est-ce qu’il y a des symptômes ?

Un peu mal à la gorge.

Au retour, il faut s’assurer d’avoir les mains propres après être allé aux toilettes.
Il faut faire pipi assis.
"""

s

classifier = pipeline("text-classification")
s = "Si la douleur descend dans la nuque, c'est normal car le centre de vision est situé ici derrière la nuque."

t = GoogleTranslator(source="fr",target="en").translate(s)

t

outputs = classifier (t)

outputs

reader = pipeline("question-answering")

question = "Why do I put a rubber band ?"

outputs = reader(question=question, context=t)

model_name = 'qanastek/51-languages-classifier'

outputs

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
stopwords = set(nltk.corpus.stopwords.words('english')) | set(nltk.corpus.stopwords.words('french'))

s="Si la douleur descend dans la nuque, c'est normal car le centre de vision est situé ici derrière la nuque."

from sklearn.feature_extraction.text import CountVectorizer

n_gram_range = (1, 1)
stop_words = stopwords.words('english') + stopwords.words('french')

# Extract candidate words/phrases
count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([s])
candidates = count.get_feature_names()

candidates

"""Language identification 
[Hugging Face](https://huggingface.co/qanastek/51-languages-classifier)
"""



"""#FIN pour l'instant"""





idx

airtable_service.get('reczF0C8BTkRcbzxL')

record['fields']['Résumé']

df_vk_ec.dtypes

df_ec_meta.dtypes



df_vk_ec.dtypes

df_ec_meta

df_vk_ec.loc[~df_vk_ec["ID"].isin(df_ec_meta["ID"])]

df_vk_ec.dtypes



vk_filter = ["rus"]
# on itère sur les langues choisies pour le run
for FILTER_LANG in vk_filter:
  # on itère sur le panier de phrases en français
  for index, row in df_ec_fr.iterrows():
    key_a = row["ID"]
    key_b = key_a[:5] + FILTER_LANG.upper() + "-" + key_a[9:]
    # on cherche si une traduction IA est déjà faite pour cette langue
    

    print (key_b)

# concaténation des dataframes
frames =[]
frames.append(df_ec_nlp)

# on fait une passe par langue et par langue pivot
vk_filter = ["eng"]
for FILTER_LANG in vk_filter:
  for dic in dict_lang_vendor:
    language = dic["language"]
    iso_code = dic["iso-code"]
    # Si la langue est celle du filtre ?
    if language == FILTER_LANG:
      # Pour chaque code vendor qui gère la langue 
      for vc in dic["vendor-code"]:
        df_return = call_nlp_language_vendor (FILTER_LANG, iso_code, vc, FILTER_PIVOT)
        frames.append(df_return)

"""# FIN IA"""

# concaténation des dataframes
frames =[]
frames.append(df_corpus_nlp)
frames.append(df_return)
#frames.append(df_deepl)
df_langdeck_ai = pd.concat(frames)
# on uniformise le type
df_langdeck_ai['translation_ai_source'] = df_langdeck_ai['translation_ai_source'].astype(str)
# on supprime les doublons
df_langdeck_ai = df_langdeck_ai.drop_duplicates(subset=['uid'], keep='last')
# on supprime les codes ai == 0
df_langdeck_ai = df_langdeck_ai.loc[df_langdeck_ai["translation_ai_source"]!="0"]

# dataframe temporaire pour chaque appel IA
ai = "12"
df_langdeck_ai_temp = df_langdeck.copy()

# ajout d'un uid par concaténation des 3 clés
df_langdeck_ai_temp["uid"] = df_langdeck_ai_temp["hash"].map(str) + ai 
df_langdeck_ai_temp = df_langdeck_ai_temp [["uid","hash","maitre","expression","language","translation","translation_pivot","translation_state","translation_ai_source"]]
df_langdeck_ai_temp["translation_ai"]=""

# on exclut les entrées déjà renseignées par l'iA
df_langdeck_ai_temp = df_langdeck_ai_temp.loc[(~df_langdeck_ai_temp["uid"].isin(df_corpus_nlp["uid"])) & (df_langdeck_ai_temp["language"]==filter)]
# on appelle la traduction IA si :
# - le uid n'existe pas dans la DF IA df_corpus_nlp
# - le filtre est positionné

df_langdeck_ai_temp

# on appelle la traduction IA si :
# - le uid n'existe pas dans la DF IA df_corpus_nlp
# - le filtre est positionné
if len(df_langdeck_ai_temp) > 0:
  from deep_translator import GoogleTranslator
  df_langdeck_ai_temp[["translation_ai","translation_state","translation_ai_source"]] = df_langdeck_ai_temp[["translation","translation_pivot","language","translation_state"]].apply(
      lambda x:translation_google(x[1],x[2], filter, ai), 
      axis=1, 
      result_type="expand")
df_google = df_langdeck_ai_temp.copy()

"""## Appel DeepL"""

# dataframe temporaire pour chaque appel IA
df_langdeck_ai_temp = df_langdeck.copy()
ai = "14"
# ajout d'un uid par concaténation des 3 clés
df_langdeck_ai_temp["uid"] = df_langdeck_ai_temp["hash"].map(str) + ai 
df_langdeck_ai_temp = df_langdeck_ai_temp [["uid","hash","maitre","expression","language","translation","translation_pivot","translation_state","translation_ai_source"]]
df_langdeck_ai_temp["translation_ai"]=""

# on exclut les entrées déjà renseignées par l'iA
df_langdeck_ai_temp = df_langdeck_ai_temp.loc[(~df_langdeck_ai_temp["uid"].isin(df_corpus_nlp["uid"])) & (df_langdeck_ai_temp["language"]==filter)]
# on appelle la traduction IA si :
# - le uid n'existe pas dans la DF IA df_corpus_nlp
# - le filtre est positionné

df_langdeck_ai_temp

from deep_translator import DeeplTranslator
df_langdeck_ai_temp[["translation_ai","translation_state","translation_ai_source"]] = df_langdeck_ai_temp[["translation","translation_pivot","language","translation_state"]].apply(
    lambda x:translation_deepl(x[1],x[2], filter, ai), 
    axis=1, 
    result_type="expand")
df_deepl = df_langdeck_ai_temp.copy()

"""## On sauvegarde"""

from deep_translator import PonsTranslator
filter = "hun"
ai = 13
df_langdeck_ai_temp[["translation_ai","translation_state","translation_ai_source"]] = df_langdeck_ai_temp[["translation","translation_pivot","language","translation_state"]].apply(
    lambda x:translation_pons(x[1],x[2], filter, ai), 
    axis=1, 
    result_type="expand")
df_pons = df_langdeck_ai_temp.copy()

"""### Ajout du dataset Google dans le dataset global IA"""

frames =[]
frames.append(df_google)
frames.append(df_pons)
df_langdeck_ai = pd.concat(frames)

# ajout d'un uid par concaténation des 3 clés
df_langdeck_ai["uid"] = df_langdeck_ai["hash"].map(str) + df_langdeck_ai["translation_ai_source"].map(str)

df_langdeck_ai.loc[(df_langdeck_ai["language"]=="hun") & (df_langdeck_ai["hash"]=="30579682hun")]

def fetch_deep_translator(hashid, language, vendor-name_code, row):
  iso_code = iso_codes[language]
  ai_dict = next(filter(lambda x: x.get('vendor-code') == vendor-name_code,deep_translator_vendor-name_codes))
  vendor-name = ai_dict["vendor-name"]
  pivot = ai_dict["pivot"]
  if pivot == "en":
    pivot_expr = row["translation_pivot"]
  else:
    pivot_expr = row["expression"]
  message = "Appel de l'IA Deep " + vendor-name + " pour la langue " + iso_code + " pour l'expression " + pivot_expr


  df_langdeck_ai_temp[["translation_ai","translation_state","translation_ai_source"]] = df_langdeck_ai_temp[["translation","translation_pivot","language","translation_state"]].apply(
      lambda x:translation_google(x[1],x[2], filter, ai), 
      axis=1, 
      result_type="expand")

  return message

hash_ai = "7c15835bukr12"
len(df_corpus_nlp.loc[df_corpus_nlp["hash_ai"]==hash_ai])

filtre = "ams"
vc = "11"
for key, value in df_langdeck.iterrows():
  if value["language"]==filtre:
    # on recherche la clé hash+ia dana la table deep
    hash_ai = value["hash"]+vc
    found = df_corpus_nlp.loc[df_corpus_nlp["hash_ai"]==hash_ai]
    # pas trouvé, on passe la ligne pour appel ia
    if len(found)==0:
      print ("Recherche de " +hash_ai)
      print (fetch_deep_translator(value["hash"], value["language"], vc, value))



translated = GoogleTranslator(source='auto', target='de').translate("keep it up, you are awesome")  # output -> Weiter so, du bist großartig







"""Google Translator

Pons
"""

'''
filter = "rou"
ai = 11
# algorithme :
# si state==1(pending) ou 2(validé) => pas d'appel IA
# si state==0 => appel ia
df_langdeck[["translation","translation_state","translation_ai_source"]] = df_langdeck[["translation","translation_pivot","language","translation_state"]].apply(
    lambda x:fetch_deep_tlr_txtai(x[1],x[2], filter, ai) 
    if x[3]==0 
    else (x[0],x[3],0), 
    axis=1, 
    result_type="expand")
'''

df_langdeck_ai_temp["hash_ai"]=df_langdeck_ai_temp[["hash","translation_ai_source"]].apply(lambda x:x[0]+str(x[1]), axis=1)

df_langdeck_ai_temp = df_langdeck_ai_temp.drop(df_langdeck_ai_temp[df_langdeck_ai_temp["translation_ai_source"]==0].index)

frames = [df_langdeck_ai, df_langdeck_ai_temp]
df_langdeck_ai = pd.concat(frames)

df_langdeck_ai.loc[df_langdeck_ai["translation_ai_source"]==13]

try:
  t_corpus_all_entries = wb.worksheet("t_translate_deep")
  wb.del_worksheet(t_corpus_all_entries)
except:
  print ("Onglet inexistant !")

wb.add_worksheet("t_translate_deep", 1, 1)
export_sheet = wb.worksheet("t_translate_deep")
set_with_dataframe(export_sheet, df_langdeck_ai)

#df_langdeck["translation_ai_source"] = np.nan
df_langdeck["translation_score"] = df_langdeck["translation_state"].apply(lambda x: (1.0 if x==1 else 0.5 if x==2 else 0.0))
df_langdeck["translation_state_txt"] = df_langdeck["translation_state"].apply(lambda x: ("OK" if x==1 else "Pending" if x==2 else "Reject"))

try:
  t_corpus_all_entries = wb.worksheet("t_corpus_all_entries")
  wb.del_worksheet(t_corpus_all_entries)
except:
  print ("Onglet inexistant !")

wb.add_worksheet("t_corpus_all_entries", 1, 1)
export_sheet = wb.worksheet("t_corpus_all_entries")
set_with_dataframe(export_sheet, df_langdeck)

"""# Vue Lexique (deprecated)"""

t_lexique = wb.worksheet("t_lexicon")
# on recharge avec les nouvelles données

"""### nouvelles colonnes Thèmes"""

# on crée une liste des valeurs uniques des 3 colonnes de thèmes
frames = [df_langdeck["theme_1"],df_langdeck["theme_2"],df_langdeck["theme_3"]]
vk_theme = list(set(pd.concat(frames).to_list()))
vk_theme.remove('')

# on ajoute les colonnes à partir de la liste et on valorise T/F
df_lexique = df_langdeck[["hash","maitre","ordre","expression","language","language_fulltext","theme_1","theme_2","theme_3"]].copy()
df_lexique = pd.concat([df_lexique, pd.DataFrame(columns = vk_theme)])
for item in vk_theme:
  df_lexique[item] = df_lexique[["theme_1","theme_2","theme_3"]].apply(lambda x:True if (x[0]==item or x[1]==item or x[2]==item) else False, axis=1)

frames=[]
df=pd.DataFrame()
frames.append(df)
# pour chaque thème on ajoute les expressions trouvées
for item in vk_theme:
  df_temp = df_lexique.loc[df_lexique[item] == True]
  df_temp.insert(0,'theme', item)
  df_temp.insert(0,'section', df_temp["ordre"].apply(lambda x:("Vocabulaire" if x==1 else "Expression" if x==2 else "Phrase" if x>=3 else "NONE")))
  df_temp.insert(0,'theme_hash', df_temp["hash"].apply(lambda x:"".join(item.split())+x))
  frames.append(df_temp)
  # on concatène le tout dans un new dataframe
  df_langdeck_theme = pd.concat(frames)

len(df_langdeck_theme)

df_langdeck_theme.iloc[1]

try:
  tbl = wb.worksheet("t_voc_synth")
  wb.del_worksheet(tbl)
except:
  print ("Onglet inexistant !")

wb.add_worksheet("t_voc_synth", 1, 1)
export_sheet = wb.worksheet("t_voc_synth")
set_with_dataframe(export_sheet, df_langdeck)

try:
  t_lexique = wb.worksheet("t_lexicon")
  wb.del_worksheet(t_lexique)
except:
  print ("Onglet inexistant !")

wb.add_worksheet("t_lexicon", 1, 1)
export_sheet = wb.worksheet("t_lexicon")
set_with_dataframe(export_sheet, df_langdeck_theme)

"""# FIN pour l'instant

# Appel IA Google Translate
"""



# txtAI
translation = translate("Forearm", "ar")
translation

def txtai_alpha (input, iso_code):
  try:
    translation = translate(input, "ar")
  except Exception as e:
    translation = ""
    pass
  return translation, 2

df_txtai = df_langdeck.loc[(df_langdeck["language"]=="ams") & (df_langdeck["translation_state"]==0) & (df_langdeck["idx"]=="B")]

df_txtai

df_txtai[["translation","translation_state"]]=df_txtai[["expression","translation_state"]].apply(lambda x: txtai_alpha(x[0], "ar") if x[1]==0 else "", axis=1, result_type="expand")

df_txtai

"""## Mise à jour inutile dans Langdeck !!"""

df_airtable_for_langdeck = get_df_airtable_for_langdeck()
df_airtable_for_langdeck["last_updated"] = df_airtable_for_langdeck[["hash","unixtime"]].apply(lambda x:compare_langdeck_time(x[0],x[1]), axis=1) 
df_newly_updated_in_airtable = df_airtable_for_langdeck.loc[df_airtable_for_langdeck["last_updated"]]

"""### Récupération des audios"""

df_newly_updated_in_airtable

"""## Mise à jour dans Langdeck"""

l = len(df_newly_updated_in_airtable)
# on parcoure la table cible
for i in range (2, l):
  # un enregistrement
  tuple_list = t_corpus.row_values(i)
  # on recherche la clé dans le dataframe source
  t_ = df_airtable_for_langdeck.loc[(df_airtable_for_langdeck['uid']==tuple_list[0]) & (df_airtable_for_langdeck['language']==tuple_list[6])]
  # verification du hash
  if len(t_)==1:
    t_corpus.update_cell(i, 9, t_["hash"].values[0])
  # comparaison des ts (A=AT;B=GG)
  ts_a = pd.to_datetime(t_["timestamp"].values[0], "%Y-%m-%d %H:%M:%S")
  ts_b = datetime.datetime.strptime(tuple_list[5], "%Y-%m-%d %H:%M:%S")
  # print (t_["hash"].values[0] + ": Airtable ts " + str(ts_a) + " GG ts " + str(ts_b))
  if ts_a > ts_b:
    # cas Airtable + récent => update dans Langdeck
    t_corpus.update_cell(i, 2, t_["maitre"].values[0])
    t_corpus.update_cell(i, 3, int(t_["ordre"].values[0]))
    t_corpus.update_cell(i, 4, t_["expression"].values[0])
    t_corpus.update_cell(i, 5, t_["idx"].values[0])
    if not pd.isnull(t_["translation"].values[0]):
      t_corpus.update_cell(i, 8, t_["translation"].values[0])    
  elif ts_b > ts_a:
    # cas Langdeck + récent => update translation dans AT uniqumeent !
    record = airtable.match('UID', tuple_list[0])
    if bool(record) is True:
      if not pd.isnull(tuple_list[7]):
        aDict = {}
        aDict[tuple_list[6]] = tuple_list[7]
        airtable.update(record['id'], aDict)
        print (record['id'])

frames = [df_airtable_for_langdeck, df_langdeck]
result = pd.concat(frames, keys=["hash"])
result.drop_duplicates(subset=['hash'], inplace=True, keep='last')

result

df_airtable_for_langdeck[(df_airtable_for_langdeck['hash'].isin(df_langdeck['hash']) & ~df_airtable_for_langdeck['timestamp'].isin(df_langdeck['timestamp']) )]

set_with_dataframe(t_corpus, result)



"""END

### Vocabulaire avec groupement :


1.   Index alpha
2.   Mot clé
3.   Ordre
"""

vk_voc = airtable.get_all(view='Vocabulary', sort=['idx_cor','Maitre','Ordre'])
df_voc = pd.DataFrame.from_records((r['fields'] for r in vk_voc))
df_voc = df_voc[['idx_cor','Maitre','Ordre','Proposition','CATEGRAM','TYPE','Thématique']]

"""### Traductions avec groupement :


1.   Index alpha
2.   Mot clé
3.   Ordre
"""

df_voc_trad = airtable.get_all(view='Traductions', sort=['idx_cor','Maitre','Ordre'])
df_voc_trad = pd.DataFrame.from_records((r['fields'] for r in df_voc_trad))
df_voc_trad = df_voc_trad[['idx_cor','Maitre','Ordre','Proposition','CATEGRAM','TYPE','Thématique',
             'eng','ams','tur','rus','ukr','rou','hun','tig','alb','geo','arm',
             'dar','pst','esp','pol']]

# Corpus_consolidation
uri = "https://docs.google.com/spreadsheets/d/1cSnSnnQunL-I7hpyYkIqbtuGr55unVJ3Qe8URSrjTr8/edit?usp=sharing"

wb = get_CCDB_wb(uri)
db_cc = get_CCDB_data (wb, 0)

nom_onglet = 'Lexique'
# si l'onglet existe déjà
try:
  ws = wb.worksheet(nom_onglet)
  wb.del_worksheet(ws)
except:
  print ("Onglet inexistant !")

wb.add_worksheet(nom_onglet, 1, 1)
export_sheet = wb.worksheet(nom_onglet)
set_with_dataframe(export_sheet, df_voc_trad)

grouped = df_voc.groupby(['idx_cor','Maitre'])
'''
for name, group in grouped:
  print (name)
  print (group["Proposition"])
'''

"""Recherche des UID manquants dans Airtable"""

def airtable_get_missing_uids(df):
  return df.loc[pd.isna(df["UID"])]

"""Recherche dans Airtable par Séquence"""

def airtable_get_records_by_sequence(df, sequence):
  return df.loc[df["Séquence"]==sequence]

#df_vkat.loc[df_vkat["Séquence"]=="1. Présentation + antécédents"]

airtable_get_missing_uids(df_vkat)

"""Rceherche des termes présents dans Central, présents dans AIRTBL mais sans UID"""

vk_at_trad = airtable.get_all(view='Traductions',sort='Proposition')
df_vk_at_trad = pd.DataFrame.from_records((r['fields'] for r in vk_at_trad))
df_vk_at_trad = df_vk_at_trad[["Proposition","idx_cor","UID"]]
df_vk_at_trad_notna = df_vk_at_trad.loc[pd.notna(df_vk_at_trad["UID"])]
df_vk_at_trad_na = df_vk_at_trad.loc[pd.isna(df_vk_at_trad["UID"])]

"""### merge côté AT : avec les UID présents """

df_merged = pd.merge(df_cc_global, df_vk_at_trad_notna, how="right", left_on='uid', right_on='UID')

"""## On complète avec les traductions"""

for index, row in df_merged.iterrows():
  if row["uid"] is not None:
    record = airtable.match('UID', row["uid"])
    aDict = {}
    for i in vk_languages:
      key = i["trigramme"]
      aDict[key] = row[key]
      #print (record['id'] +" " + row["eng"])
    airtable.update(record['id'], aDict)

"""## On complète avec les UID manquants"""

df_vk_at_trad_na

df_merged = pd.merge(df_cc_global, df_vk_at_trad_na, how="right", left_on='expression', right_on='Proposition')

df_merged = df_merged.loc[pd.notna(df_merged["uid"])]

for index, row in df_merged.iterrows():
  if row["uid"] is not None:
    record = airtable.match('Proposition', row["expression"])
    aDict = {}
    key = "UID"
    aDict[key] = row["uid"]
    for i in vk_languages:
      key = i["trigramme"]
      aDict[key] = row[key]
      #print (record['id'] +" " + row["eng"])
    airtable.update(record['id'], aDict)

"""## On vide toute la base AT dans Central Consolidation"""

vk_at = airtable.get_all(view='Admin',sort='Proposition')
df_vkat = pd.DataFrame.from_records((r['fields'] for r in vk_at))

# Corpus_consolidation
uri = "https://docs.google.com/spreadsheets/d/1cSnSnnQunL-I7hpyYkIqbtuGr55unVJ3Qe8URSrjTr8/edit?usp=sharing"

wb = get_CCDB_wb(uri)
db_cc = get_CCDB_data (wb, 0)

nom_onglet = 'Termes'
# si l'onglet existe déjà
try:
  ws = wb.worksheet(nom_onglet)
  wb.del_worksheet(ws)
except:
  print ("Onglet inexistant !")

wb.add_worksheet(nom_onglet, 1, 1)
export_sheet = wb.worksheet(nom_onglet)
set_with_dataframe(export_sheet, df_vkat)

"""END"""

record = airtable.match('UID', '30579682')
fields = {'eng': 'Accident'}
airtable.update(record['id'], fields)

record

def airtable_table_get(uri, params, headers):
  airtable_records = []
  run = True
  while run is True:
    response = requests.get(uri, params=params, headers=headers)
    airtable_response = response.json()
    airtable_records += (airtable_response['records'])
    if 'offset' in airtable_response:
      run = True
      params = (('offset', airtable_response['offset']),)
    else:
      run = False
  return airtable_records

"""# Table Corpus"""

url = "https://api.airtable.com/v0/" + base_id + "/" + table_name

"""### On lit tous les enregistrements, puis on met dans un dataframe

# Nouvelle section
"""

airtable_records = airtable_table_get(url, params, headers)

airtable_rows = [] 
airtable_index = []
for record in airtable_records:
  airtable_rows.append(record['fields'])
  airtable_index.append(record['id'])
df = pd.DataFrame(airtable_rows, index=airtable_index)
df = df [['Proposition','UID', 'Name ID', 'DEPENDANCE', 'idx', 'idx_cor','Statut','Thématique']]
df.sort_values(by=['Proposition'], ascending=True, inplace=True)

df.head(10)

def lookup_uid (uid):
  try:
    record = df_cc_fr.loc[df_cc_fr["uid"]==uid]["index"].item()
  except Exception as e:
    list_no_uid.append(uid)
    return uid
  if record is not None:
    return record

x = df_cc_fr.loc[df_cc_fr["uid"]=="72f0e8db"]["index"].item()

list_no_uid = [] # uid non présents côté central = mots à créer
df['UID'] = df['UID'].apply(lambda x: make_unique_id() if pd.isna(x) else x)
df.idx_cor = df['UID'].apply(lambda x: lookup_uid(x))

"""convert the dataframe to a dictionary of dictionaries."""

df.tail()

dict_records = df.tail().to_dict(orient="index")

for record_id, upload_data in dict_records.items():
  record_url = url + "/" + record_id
  upload_dict = {"records" : [{"fields" : upload_data}]}
  upload_json = json.dumps(upload_dict)
  requests.post(url, data=upload_json, headers=headers)

headers

requests.patch(url, data=upload_json, headers=headers)

upload_json='{"records": [{"fields": {"Proposition": "OEDEME", "id": "reczy8Os5Hc3qZ1yA"}}]}'

requests.post(record_url, data=upload_json, headers=headers)

def fmt_table(wb, nom_onglet):
  # onglet
  ws = wb.worksheet(nom_onglet)

  # Format de la partie gauche
  fmt = cellFormat(
      backgroundColor=color(0.91, 0.96, 0.93),
      textFormat=textFormat(bold=False, foregroundColor=color(0,0,0), fontSize='10'),
      horizontalAlignment='LEFT'
      )
  format_cell_range(ws, 'A:', fmt)

  # format de l'entete
  fmt = cellFormat(
      backgroundColor=color(0.7725,0.8431,0.7922),
      textFormat=textFormat(bold=True, foregroundColor=color(0,0,0), fontSize='10'),
      horizontalAlignment='LEFT'
      )
  format_cell_range(ws, '1', fmt)
  
  # format de l'auto wrap
  fmt = cellFormat(
      wrapStrategy='WRAP'
      )
  format_cell_range(ws, 'B:C', fmt)

  # on gèle l'entete et les colonnes de gauche
  set_frozen(ws, rows=1, cols=1)
  set_column_width(ws, 'A:C', 500)

nom_onglet = 'Liste'
# si l'onglet existe déjà
try:
  ws = wb.worksheet(nom_onglet)
  wb.del_worksheet(ws)
except:
  print ("Onglet inexistant !")

wb.add_worksheet(nom_onglet, 1, 1)
export_sheet = wb.worksheet(nom_onglet)
set_with_dataframe(export_sheet, df)
fmt_table(wb,nom_onglet)

table_name = 'Structure'
url = "https://api.airtable.com/v0/" + base_id + "/" + table_name

df

